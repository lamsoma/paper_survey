・単語ベクトルに基づく新たな meaning-frequency law の検証
### 論文リンク
https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/P1-20.pdf
### 著者/所属機関
永田亮
### 投稿年
2024
## 概要：
埋め込みに基づき、単語頻度と語義数に関する法則を示す。
## 研究背景
meaning-frequency law とは，頻度が高い単語ほど， 語義数が多くなるという経験則である.より形式的 には，単語の頻度を 𝑓 ，語義数を 𝑚 としたとき，
log(𝑚) = 𝛿 log( 𝑓 ) + 𝑐 (1)
という冪乗則が成り立つというものである
・語義数を決定するのが難しい。
・機能語や高頻度語を考慮しないことが多い
## 提案手法
語義の豊富さをベクトル方向の多様さで定義
m=(1-l^2)/l
埋め込み空間において
## 実験
コーパスを問わず，式 (1) に概ね回帰することかがわかった
年々言葉が多義的になっていることも示唆された。
また、小さいモデルではこのようなことは成り立たず、モデルの評価にも使えそう
## 感想
着眼点が面白い論文。mを単語ベクトルで代替できますよと言う話で止まらず、年々言葉が多義的になっているのも直感に即していて面白い。
## 参考

平均プーリングによる文埋め込みの再検討: 平均は点群の要約として十分か?
### 論文リンク
https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/A10-4.pdf
### 著者/所属機関
原知正
### 投稿年
2024
## 概要：
平均プーリングは
## 研究背景

## 提案手法
点群距離WMDを用いて比較
「点群としては明らかに異なるのに，平均プーリングをすると同じ位置に埋め込まれてしまう」という現象を，「点群間の WMD は 大きいが平均プーリング後の L2 距離は小さい」と いう条件で探していく.
## 実験
平均プーリングで測った相対的な文類似度 − WMD で測った相対的な文類似度」を表しており，これが 0.3 を超えることは，5 段階(しかない)STS スコア が 3 ずれること，
つまり平均プーリングと WMD で 類似度の推定値に大きな乖離があることを意味する.
このような文ペアを機械的に抽出すると，デー タセットの中に 201 件存在し，これはデータセット 全体の 3.6%に該当する.
## 感想
平均プーリングでいいの？と言うのはすごい素直な疑問でそれをしっかり検証できているのはそれだけで有用
## 参考

・テキスト生成モデルを利用したデータセット蒸留
### 論文リンク

### 著者/所属機関

### 投稿年
2024
## 概要：
学習用のデータセットを蒸留させていいものを使うようにしましょうと言うお話。
## 研究背景
大規模なモデルの学習には膨大な時間と計算 資源を要するため，新しいモデルの開発はおろか，追学習さえ困難となっている.
このような背景から，訓練データセット中の知識を蒸留することで， 学習効果を保持しつつ訓練データセットを圧縮するデータセット蒸留 [1] が近年注目されている.
データセット蒸留は，モデルを効率的に学習可能な少量 の人工的なデータを作成することで，モデルの学習 コストの大幅な削減を実現する.
## 提案手法
テキスト生成モデルの生成サンプルで計算されたモデルの勾配と、訓練データセット中の実サンプルで計算された平均勾配が一致するように学習する。
## 実験
まず，単に訓練データの生成を学習しただけ の追学習前のモデルでは，コアセット選択の手法の 性能を明らかに下回った.
これは直感の通りではあるが，テキスト生成モデルの生成サンプルの質が元の訓練データセットの実サンプルと比較して低いことを示唆している.
一方で，勾配の一致度に基づく追学習を適用することで性能は大幅に改善され，コアセット選択の手法をほぼ全ての設定で上回った.
Learner以外のモデルでも性能向上が見られた
## 感想
生成したデータセットを蒸留しようと言う試みがあまり見かけなくて面白い。
元データがあることを前提としているんので、自分の研究に直接は活かせないが、工夫次第ではベースラインを越えれそう。
## 参考

Style SimSCE: SNSユーザ同一性に基づく対照学習によるスタイル類似性を捉えた文ベクトルの獲得
### 論文リンク

### 著者/所属機関

### 投稿年
2024
## 概要：

## 研究背景

## 提案手法

## 実験

## 感想

## 参考

・テキスト生成モデルを利用したデータセット蒸留

・単一トークン適応による大規模言語モデルに基づく文埋め込み
